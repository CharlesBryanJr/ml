{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Manipulation in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here let's look at a different dataset that will allow us to really dive into some meaningful visualizations. This data set is publically available, but it is also part of a Kaggle competition.\n",
    "\n",
    "You can get the data from here: https://www.kaggle.com/c/titanic-gettingStarted or you can use the code below to load the data from GitHub.\n",
    "\n",
    "There are lots of iPython notebooks for looking at the Titanic data. Check them out and see if you like any better than this one!\n",
    "\n",
    "When going through visualization options, I recommend the following steps:\n",
    "- Would you like the visual to be interactive?\n",
    "  - Yes, Does it have a lot of data?\n",
    "    - No, Use plotly or bokeh\n",
    "    - Yes, sub-sample and then use plotly/bokeh\n",
    "    - Yes, think about using Turi for large data\n",
    "  - No, Does seaborn have a built-in function for plotting?\n",
    "    - Yes, use seaborn\n",
    "    - No, Does Pandas support the visual?\n",
    "      - Yes, use pandas\n",
    "      - No, use low level matplotlib\n",
    "      \n",
    "Look at various high level plotting libraries like:\n",
    "- Altair (https://altair-viz.github.io)\n",
    "- Bokeh (http://bokeh.pydata.org/en/latest/)\n",
    "- And many others...\n",
    "\n",
    "## Adding Dependencies (for Jupyter Lab)\n",
    "- `conda install -c conda-forge missingno`\n",
    "- `conda install nodejs`\n",
    "- `jupyter labextension install @jupyterlab/plotly-extension`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Titanic Data for Example Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Titanic dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',np.__version__)\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/eclarson/DataMiningNotebooks/master/data/titanic.csv') # read in the csv file\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the describe function defaults to using only some variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print('===========')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions we might want to ask:\n",
    "- What percentage of passengers survived the Titanic disaster?\n",
    "- What percentage survived in each class (first, coach, etc.)?\n",
    "- How many people traveled in each class? How many classes are there?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentage of individuals that survived on the titanic\n",
    "sum(df.Survived==1)/len(df)*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets aggregate by class and count survival rates\n",
    "df_grouped = df.groupby(by='Pclass')\n",
    "\n",
    "for val,grp in df_grouped:\n",
    "    print(f'There were {len(grp)} people traveling in {val} class.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the groupby function with a data column\n",
    "print(df_grouped['Survived'].sum())\n",
    "print('---------------------------------------')\n",
    "print(df_grouped.Survived.count())\n",
    "print('---------------------------------------')\n",
    "print(df_grouped.Survived.sum() / df_grouped.Survived.count())\n",
    "\n",
    "# might there be a better way of displaying this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Exercise üìù: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Exercise: Create code for calculating the std error\n",
    "# std / sqrt(N)\n",
    "\n",
    "df_grouped.Survived.std() / np.sqrt(df_grouped.Survived.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "# Cleaning the Dataset\n",
    "Let's start by visualizing some of the missing data in this dataset. We will use the `missingno` package to help visualize where the data contains `NaNs`. This is a great tool for looking at nan values and how we might go about filling in the values. \n",
    "\n",
    "For this visualization, we can use a visualization library called `missingno` that hs many types of visuals for looking at missing data in a dataframe. I particularly like the `matrix` visualization, but there are many more to explore:\n",
    "- https://github.com/ResidentMario/missingno\n",
    "\n",
    "### Plot Type One: Filter Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this python magics will allow plot to be embedded into the notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "\n",
    "# External package: conda install missingno \n",
    "import missingno as mn\n",
    "\n",
    "mn.matrix(df)\n",
    "plt.title(\"Not Sorted\",fontsize=22)\n",
    "\n",
    "# plt.figure()\n",
    "# mn.matrix(df.sort_values(by=[\"Cabin\",\"Age\"]))\n",
    "# plt.title(\"Sorted\",fontsize=22)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean the dataset a little before moving on\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['PassengerId','Name','Cabin','Ticket']:\n",
    "    if col in df:\n",
    "        del df[col]\n",
    "        \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Techniques \n",
    "Let's compare two different techniques from lecture on how to fill in missing data. Recall that imputation should be done with a great deal of caution. Here, the Age variable seems to be missing about 15% of the values. That might be too many to impute. Let's try two methods of imputation on the Age variable: \n",
    "- Split-Imput-Combine (SIC)\n",
    "- K-Nearest Neighbor Imputation (KNN)\n",
    "\n",
    "**Self Test ML2a.0** \n",
    "What is a difference between the Split-Impute-Combine (SIC) Technique and the K-Nearest Neighbor (KNN) Imputation Technique:\n",
    "- A. SIC imputes values based on distances, KNN does not\n",
    "- B. SIC uses discrete groupings of instances, KNN does not\n",
    "- C. KNN does not use statistics like median, mode, or mean to fill in values, SIC does\n",
    "- D. There is no difference in the two techniques \n",
    "\n",
    "### Split-Impute-Combine in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for split, impute, combine\n",
    "#     let's clean the dataset a little before moving on\n",
    "\n",
    "\n",
    "# Impute some missing values, grouped by their Pclass and SibSp numbers, \n",
    "# then use this grouping to fill the data set in each group, then transform back\n",
    "\n",
    "df_grouped = df.groupby(by=['Pclass','SibSp','Parch']) # perform the grouping of thing related to 'age'\n",
    "func = lambda grp: grp.fillna(grp.mean()) # within groups, fill using median (define function to do this)\n",
    "numeric_columns = ['Survived','Age','Parch','SibSp','Pclass','Fare'] # only transform numeric columns\n",
    "df_imputed_sic = df_grouped[numeric_columns].transform(func) # apply impute and transform the data back\n",
    "\n",
    "# Extra step: fill any object columns that could not be transformed\n",
    "col_deleted = list( set(df.columns) - set(df_imputed_sic.columns)) # in case the median operation deleted columns\n",
    "df_imputed_sic[col_deleted] = df[col_deleted]\n",
    "\n",
    "# drop any rows that still had missing values after grouped imputation\n",
    "df_imputed_sic.dropna(inplace=True)\n",
    "\n",
    "# 5. Rearrange the columns\n",
    "df_imputed_sic = df_imputed_sic[['Survived','Age','Sex','Parch','SibSp','Pclass','Fare','Embarked']]\n",
    "df_imputed_sic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Imputation with Scikit-learn\n",
    "Now let's try to fill in the Age variable by selecting the 3 nearest data points to the given observation. Here, we can use additional variables in the distance calculation, as compared to the need for discrete variable in the split-impute-combine method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute based upon the K closest samples (rows)\n",
    "# our first introduction to sklearn!!!\n",
    "from sklearn.impute import KNNImputer\n",
    "import copy\n",
    "\n",
    "# get object for imputation\n",
    "knn_obj = KNNImputer(n_neighbors=3)\n",
    "\n",
    "features_to_use = ['Survived', 'Pclass','Age','SibSp','Parch', 'Fare']\n",
    "\n",
    "# create a numpy matrix from pandas numeric values to impute\n",
    "temp = df[features_to_use].to_numpy()\n",
    "\n",
    "# use sklearn imputation object\n",
    "knn_obj.fit(temp) # fit the object\n",
    "temp_imputed = knn_obj.transform(temp) # transform all mssing data\n",
    "#    could have also done:\n",
    "# temp_imputed = knn_obj.fit_transform(temp)\n",
    "\n",
    "# this is VERY IMPORTANT, make a deep copy, not just a reference to the object\n",
    "# otherwise both data frames will be manipulated\n",
    "df_imputed = copy.deepcopy(df) # not just an alias\n",
    "df_imputed[features_to_use] = temp_imputed\n",
    "df_imputed.dropna(inplace=True)\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties of the imputer after fitting\n",
    "print(knn_obj.n_features_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Imputation Distributions \n",
    "Now let's see whihc imputation method changed the overall histogram the least. **Do you see anything in the plots below that would give preference in one method over another?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show some very basic plotting to be sure the data looks about the same\n",
    "# Which imputation did better? The Split-Apply-Combine, or Nearest Neighbor Imputer? \n",
    "f = plt.figure(figsize=(16,5))\n",
    "\n",
    "bin_num = 30\n",
    "plt.subplot(1,2,1)\n",
    "df_imputed_sic.Age.plot(kind='hist', alpha=0.25, \n",
    "                        label=\"Split-Impute-Combine\",\n",
    "                        bins=bin_num)\n",
    "\n",
    "df.Age.plot(kind='hist', alpha=0.25, \n",
    "                        label=\"Original\",\n",
    "                        bins=bin_num)\n",
    "plt.legend()\n",
    "plt.ylim([0, 150])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "df_imputed.Age.plot(kind='hist', alpha=0.25, \n",
    "                        label=\"KNN-Imputer\",\n",
    "                        bins=bin_num)\n",
    "\n",
    "df.Age.plot(kind='hist', alpha=0.25, \n",
    "                        label=\"Original\",\n",
    "                        bins=bin_num)\n",
    "plt.legend()\n",
    "plt.ylim([0, 150])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
