{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<style>\n",
    ".code_cell {\n",
    "   background-color: #fff;\n",
    "}\n",
    ".cm-s-ipython .CodeMirror-code * {\n",
    "   color: #000; /* Black text color */\n",
    "}\n",
    ".prompt {\n",
    "   color: #000;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter Team Member Names here (*double click to edit*):\n",
    "\n",
    "- Name 1: Charles Bryan\n",
    "- Name 2:\n",
    "- Name 3:\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment Two\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class (or right after class). The initial portion of this notebook is given before class and the remainder is given during class. Please answer the initial questions before class, to the best of your ability. Once class has started you may rework your answers as a team for the initial part of the assignment. \n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Data</a>\n",
    "* <a href=\"#ff\">Defining a Feedforward Network in Python</a>\n",
    "* <a href=\"#bp\">Back Propagation in Python</a>\n",
    "* <a href=\"#vis\">Visualizing Back Propagation</a>\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Data\n",
    "Please run the following code to read in the \"digits\" dataset from sklearn's data loading module. This data contains hand written digits for the characters 0-9.\n",
    "\n",
    "This will load the data into the variable `ds`. `ds` is a `bunch` object with fields like `ds.data` and `ds.target`. The field `ds.data` is a numpy matrix of the continuous features in the dataset. **The object is not a pandas dataframe. It is a numpy matrix.** Each row is a set of observed instances, each column is a different feature. It also has a field called `ds.target` that is an integer value we are trying to predict (i.e., a specific integer represents a specific person). Each entry in `ds.target` is a label for each row of the `ds.data` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5 # normalize the data\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape and print a few of the images in the digits dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____\n",
    "**Question 1:** For the digits dataset, what does each column in $\\mathbf{X}$ represent? What does each row in $\\mathbf{X}$ represent? What does each value in $\\mathbf{X}$ represent? What does each unique value of the target, $y$ represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter you answer here (*double click to edit*)\n",
    "\n",
    "- Column: represents a specific pixel's intensity in the 8x8 image of a digit. Since each image is 8x8 pixels and flattened into a single row, there are 64 columns in total, each corresponding to a pixel's value in the flattened image.\n",
    "- Row: represents the flattened pixel values of a single 8x8 image of a digit. When reshaped to an 8x8 matrix, it visually represents the handwritten digit.\n",
    "- Value: represents the intensity of a pixel in the image, where the value has been normalized to range between -0.5 and 0.5.\n",
    "- Unique Target in $y$: represents the actual digit that the corresponding row in X is an image of. The values in y range from 0 to 9, corresponding to the ten possible digits in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/MultiLayerNetwork.png\" width=\"500\">\n",
    "\n",
    "**Question 2:** For the digits dataset, we want to train a neural network with one hidden layer (two layers total). The hidden layer will have 30 neurons. What will be the size of the matrices and bias terms in each layer? That is, what is the size of $\\mathbf{W}^{(1)}$, $\\mathbf{b}^{(1)}$, and what is the size of $\\mathbf{W}^{(2)}$, $\\mathbf{b}^{(2)}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the sizes of the weight matrices and bias vectors for the neural network, we need to consider the following -\n",
    "- number of input features = 64 features (each pixel in an 8x8 image)\n",
    "- the number of neurons in the hidden layer = 30 neurons\n",
    "- the number of output classes = 10 neurons (corresponding to the 10 possible digit classes: 0 to 9)\n",
    "\n",
    "\n",
    "Input Layer to Hidden Layer:\n",
    "- W1 = 64 (input features) * 30 (hidden layer neurons) = 1920\n",
    "- b1 = 30 (Each neuron in the hidden layer has a bias term)\n",
    "\n",
    "Hidden Layer to Output Layer:\n",
    "- W2 = 30 (neurons in the hidden layer) x 10 (output classes) = 300\n",
    "- b2 = 10 (one bias term for each output neuron)\n",
    "\n",
    "\n",
    "Enter you answer here (*double click to edit*)\n",
    "\n",
    "- $\\mathbf{W}^{(1)}$: 1920\n",
    "- $\\mathbf{b}^{(1)}$: 30\n",
    "- $\\mathbf{W}^{(2)}$: 300\n",
    "- $\\mathbf{b}^{(2)}$: 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ff\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "\n",
    "\n",
    "# Defining a Feedforward Network\n",
    "\n",
    "Below we will setup the functions for use in a feedforward neural network **with two layers**. Take a quick look at the functions defined. There are a number of convenience functions including:\n",
    "- a function for the sigmoid calculation\n",
    "- a function to one hot encode the output\n",
    "- an initialization function for initializing the weights\n",
    "\n",
    "A few functions are not yet implemented including:\n",
    "- a `fit` function\n",
    "- a `get_gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\"\"\"\n",
    "        # Backpropagation\n",
    "        sigma3 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        sigma2 = (W2.T @ sigma3)*A2*(1-A2)\n",
    "        \n",
    "        grad1 = sigma2 @ A1.T\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        \n",
    "        # Regularization and bias\n",
    "        grad1 += (W1 * self.l2_C)\n",
    "        grad2 += (W2 * self.l2_C)\n",
    "        \n",
    "        gradb1 = np.sum(sigma2, axis=1, keepdims=True)\n",
    "        gradb2 = np.sum(sigma3, axis=1, keepdims=True)\n",
    "        \n",
    "        return grad1, grad2, gradb1, gradb2\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        self.n_output_ = np.unique(y).shape[0]  # Number of class labels\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # Feedforward\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3, Y_enc, self.W1, self.W2)\n",
    "            self.cost_.append(cost)\n",
    "            \n",
    "            # Compute gradient via backpropagation\n",
    "            grad1, grad2, gradb1, gradb2 = self._get_gradient(A1, A2, A3, Z1, Z2, Y_enc, self.W1, self.W2)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "            # Update biases\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d | Cost: %.2f' % (i+1, self.epochs, cost))\n",
    "                sys.stderr.flush()\n",
    "        \n",
    "        if print_progress:\n",
    "            sys.stderr.write('\\n')\n",
    "            sys.stderr.flush()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img src=\"https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/MultiLayerNetwork.png\" width=\"500\">\n",
    "\n",
    "Recall from the videos that all instances in $\\mathbf{X}$ can be fed into the network with a single matrix multiplication operation for each intermediate vector, $\\mathbf{a}^{(l)}$ and $\\mathbf{z}^{(l)}$. When we feed all the instances, $\\mathbf{X}$, the intermediate vectors, $\\mathbf{a}^{(l)}$ and $\\mathbf{z}^{(l)}$ get stacked together to form matrices, $\\mathbf{A}^{(l)}$ and $\\mathbf{Z}^{(l)}$. This is already done for you in the `_feedforward` function defined above.\n",
    "\n",
    "**Question 3:**\n",
    "For the digits dataset we are using and a network with 30 neurons in the hidden layer, what are the sizes of:\n",
    "- **Part A**: the intermediate vectors, $\\mathbf{a}^{(1)}$ and $\\mathbf{a}^{(2)}$\n",
    "- **Part B**: the intermediate vectors, $\\mathbf{z}^{(1)}$ and $\\mathbf{z}^{(2)}$\n",
    "- **Part C**: the intermediate matrices, $\\mathbf{A}^{(1)}$ and $\\mathbf{A}^{(2)}$\n",
    "- **Part D**: the intermediate matrices, $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter you answer here (double click)*\n",
    "\n",
    "\n",
    "A. a(1) corresponds to the activations of the hidden layer. Since there are 30 neurons in the hidden layer, a(1) will have 30 elements. a(2) corresponds to the output layer. Since are using one-hot encoding for classification for classify digits 0-9, there will be 10 output neurons. a(2) will have 10 elements.\n",
    "\n",
    "\n",
    "B. z(1) will have 30 elements, which is the same size as a(1). z(2) will have 10 elements, which is the same size as a(2).\n",
    "\n",
    "\n",
    "C. A(1) would be an m by 30 matrix, where m is the number of instances. A(2) would be an m by 10 matrix, where m is the number of instances.\n",
    "\n",
    "\n",
    "D.  Z(1) would be an m by 30 matrix.Z(2) would be an m by 10 matrix.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"bp\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "# Back Propagation in Python\n",
    "Now let's add in the back propagation steps from the video. First, we need to add in a `fit` function that will update all the trainable weights in the $\\mathbf{W}^{(l)}$ matrices. Because this is a two layer network we have layers $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$. Look at the given `fit` function written below for you. \n",
    "\n",
    "The `fit` function below will update matrices using steepest descent. And the update equation is:\n",
    "$$  w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\eta \\frac{\\partial J(\\mathbf{W})}{\\partial w_{i,j}^{(l)}}$$\n",
    "\n",
    "for each value in each matrix, $\\mathbf{W}^{(l)}$. \n",
    "\n",
    "The objective function is simply the mean squared error:\n",
    "$$ J(\\mathbf{W}) = \\sum_{k=1}^M (\\mathbf{y}^{(k)}-[\\mathbf{a}^{(L)}]^{(k)})^2 $$\n",
    "\n",
    "where $L$ is the output of the last layer. For our two layer implementation, $L=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "            Input X should be a matrix with separate instances\n",
    "            in each row, and separate features in each column.\n",
    "            The target variable, y, should be integer values \n",
    "            starting from zero, that represent the unique classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        #------------------------------------------\n",
    "        # You will update These arrays, initialized here \n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        \n",
    "        #------------------------------------------\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            # WE HAVE NOT YET WRITTEN THE GRADIENT FUNCTION YET\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, \n",
    "                                                                A2=A2, \n",
    "                                                                A3=A3, \n",
    "                                                                Z1=Z1, \n",
    "                                                                Z2=Z2, \n",
    "                                                                Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE to keep track of the \n",
    "            # magnitude of gradients grad_w1_ and grad_w2_\n",
    "            \n",
    "            # Track the magnitude of the gradient \n",
    "            #self.grad_w1_[i] = ???\n",
    "            #self.grad_w2_[i] = ???\n",
    "            \n",
    "            self.grad_w1_[i] = np.linalg.norm(gradW1)\n",
    "            self.grad_w2_[i] = np.linalg.norm(gradW2)\n",
    "            \n",
    "            #------------------------------------------\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** In the code above, add class properties (variables), `grad_w1_` and `grad_w2_` that save the average magnitude of the gradient for each layer at every epoch. That is, if you ran the `fit` function for 50 epochs, `grad_w1_` and `grad_w2_` would be 50 element vectors when training is complete.\n",
    "\n",
    "When training is completed, `grad_w1_` and `grad_w2_` should be accessible using dot notation from the class object, as shown in the example syntax below. \n",
    "\n",
    "```\n",
    "clf = TwoLayerPerceptron()\n",
    "clf.fit(X,y)\n",
    "clf.grad_w1_ \n",
    "clf.grad_w2_\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Gradient Calculation\n",
    "From the videos, recall that the the sensitivities, $\\mathbf{V}$, can be calculated as follows:\n",
    "\n",
    "$$ \\mathbf{V}^{(2)} = -2(\\mathbf{Y}-\\mathbf{A}^{(3)})*\\mathbf{A}^{(3)}*(1-\\mathbf{A}^{(3)}) $$\n",
    "$$ \\mathbf{V}^{(1)} =  \\mathbf{A}^{(2)} * (1-\\mathbf{A}^{(2)}) * [\\mathbf{W}^{(2)}]^T \\cdot \\mathbf{V}^{(2)}  $$\n",
    "\n",
    "Once we have these sensitivities, its easy to calculate the gradient of each layer. The gradient of the objective function with respect to the final layer $\\mathbf{W}^{(2)}$ can be calculated with:\n",
    "$$  \\frac{\\nabla J(\\mathbf{W})}{\\partial\\mathbf{W}^{(2)}} = \\mathbf{V}^{(2)}\\cdot [\\mathbf{A}^{(2)}]^T $$\n",
    "\n",
    "And the gradient of the objective function with respect to the first layer $\\mathbf{W}^{(2)}$ can be calculated with: \n",
    "$$  \\frac{\\nabla J(\\mathbf{W})}{\\partial\\mathbf{W}^{(1)}} = \\mathbf{V}^{(1)}\\cdot [\\mathbf{A}^{(1)}]^T $$\n",
    "\n",
    "___\n",
    "** Exercise 2:** In the code below, use numpy linear algebra functions to calculate the sensitivities at each layer, $\\mathbf{V}^{(1)}$ and $\\mathbf{V}^{(2)}$. This will complete the `_get_gradient` private method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVect(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        \n",
    "        #---------------------------------\n",
    "        # ENTER YOUR CODE HERE to\n",
    "        # calculate the sensitivities\n",
    "        # NOTE: be sure to use parentheses for correct order of operations...\n",
    "        \n",
    "        # Output layer sensitivity (V2)\n",
    "        sigma3 = A3 - Y_enc\n",
    "        V2 = sigma3 * A3 * (1 - A3)  # Derivative of the sigmoid function\n",
    "        \n",
    "        # Hidden layer sensitivity (V1)\n",
    "        sigma2 = W2.T @ V2\n",
    "        V1 = sigma2 * A2 * (1 - A2)  # Derivative of the sigmoid function \n",
    "        #---------------------------------\n",
    "        \n",
    "        # Use of sensitivity is calculated for you here\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"vis\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "# Visualizing Back Propagation\n",
    "Now let's test the code you wrote above to perform gradient calculations and save the magnitude of the gradient at each epoch. Run the code in the cell below. If it runs and the accuracy is relatively high, it is likely that your sensitivity calculation above was written correctly.\n",
    "\n",
    "*If it does not run or the accuracy is low, there is probably something wrong with the sensitivity calculation. Try to fix it before moving on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TwoLayerPerceptronVect(n_hidden=10, epochs=1500, eta=0.001)\n",
    "clf.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = clf.predict(X)\n",
    "accuracy_score(y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Visualizing the gradients\n",
    "Now let's check your calculation of the average gradient magnitude. Run the code below to visualize the average gradient magnitude versus the epochs run while training. \n",
    "\n",
    "*If the code below does not run, you likely have an error in your calculation of the average gradient magnitude.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(clf.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(clf.grad_w2_[10:]), label='w2')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** In the plot above, you should see that one layer always has a larger gradient magnitude than the other layer. \n",
    "\n",
    "- **Part A:** Which layer always has the larger magnitude? Why do you think this particular layer always has a larger magnitude? \n",
    "- **Part B:** If one layer has a larger gradient, does this also mean that the weights for that layer are training more quickly (that is, with fewer iterations)? Is that desireable for training the neural network? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter you answer here (double click to edit)*\n",
    "\n",
    "- Part A: From the plot, w2 always has a larger magnitude. w2 always has a larger magnitude because of vanishing gradients. The gradients for the earlier layers, w1, diminish as they are propagated back through the network.\n",
    "  \n",
    "- Part B: A larger gradient does not necessarily mean that the weights for that layer are training more quickly. It does indicate that the changes to the weights are larger with each iteration, which if not desireable for training the neural network. Larger gradients indicate steeper descent, they are not an end goal. The objective is to find a stable path to the minimum of the loss function, which requires careful balancing of gradients and learning rates across all layers of the network.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Exercise:** In the block of code below, come up with an adaptive scheme to mitigate the effect of unequal magnitude gradients in each layer. Your algorithm should not 'break' the optimization algorithm (*i.e.*, the accuracy should remain somewhat high). Save the magnitude of the resulting update for each layer. \n",
    "\n",
    "Explain your scheme below (that is, document what your adaptive algorithm does) and then implement your strategy below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter description here (*double click to edit*)\n",
    "\n",
    "In this adaptive scheme, the goal is to mitigate the effect of unequal magnitude gradients across the layers of a neural network. The idea is to dynamically adjust the learning rate for each layer based on certain criteria to ensure that each layer's weights are updated in a balanced manner. \n",
    "\n",
    "For preventing unequal gradients, we decided to dynamically adjust the learning rate for each layer based on the training progress and the layer's gradient magnitudes. This adaptive approach ensures that each layer's updates are balanced and effective.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronNew(TwoLayerPerceptronVect):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "            Input X should be a matrix with separate instances\n",
    "            in each row, and separate features in each column.\n",
    "            The target variable, y, should be integer values \n",
    "            starting from zero, that represent the unique classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        #------------------------------------------\n",
    "        # You will update These arrays, initialized here \n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        \n",
    "        #------------------------------------------\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, \n",
    "                                                                A2=A2, \n",
    "                                                                A3=A3, \n",
    "                                                                Z1=Z1, \n",
    "                                                                Z2=Z2, \n",
    "                                                                Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE to keep track of the \n",
    "            #    average magnitude of gradient of each layer \n",
    "            #    grad_w1_ and grad_w2_\n",
    "            #    and make the eta values for each adaptive \n",
    "            \n",
    "            \n",
    "            # Track the magnitude of the gradient \n",
    "            #self.grad_w1_[i] = ??? #(use same calculation as above)\n",
    "            #self.grad_w2_[i] = ??? #(use same calculation as above)\n",
    "            \n",
    "            self.grad_w1_[i] = np.mean(np.abs(gradW1))\n",
    "            self.grad_w2_[i] = np.mean(np.abs(gradW2))\n",
    "            \n",
    "            # eta1 = ??? #(new dynamic eta values)\n",
    "            # eta2 = ??? #(how should these be calculated???)\n",
    "\n",
    "            # Adjust learning rates dynamically (this is a simple example; you may want a more sophisticated method)\n",
    "            epsilon = 1e-8\n",
    "            eta1 = self.eta / (1 + i * epsilon)  # Example of dynamic adjustment\n",
    "            eta2 = self.eta / (1 + i * epsilon)  # Same for both layers here, but can be different\n",
    "            \n",
    "            #------------------------------------------\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE\n",
    "            # Track the magnitude of the gradient update here\n",
    "            # This should be AFTER applying your dynamic scaling\n",
    "            # That is, you SHOULD include eta here.\n",
    "            \n",
    "            #self.update_w1_[i] = ???\n",
    "            #self.update_w2_[i] = ???\n",
    "\n",
    "            # Track the magnitude of the updates after applying the dynamic learning rate\n",
    "            self.update_w1_[i] = np.mean(np.abs(eta1 * gradW1))\n",
    "            self.update_w2_[i] = np.mean(np.abs(eta2 * gradW2))\n",
    "            \n",
    "            #------------------------------------------\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "clf2 = TwoLayerPerceptronNew(n_hidden=10, epochs=1500, eta=0.001)\n",
    "clf2.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = clf2.predict(X)\n",
    "# the accuracy of the classifier should remain high!\n",
    "# Do not let your dynamic updates make the classifier worse...\n",
    "print(accuracy_score(y,yhat))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(clf2.update_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(clf2.update_w2_[10:]), label='w2')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "That's all! Please **save (make sure you saved!!!) and upload your rendered notebook** and please include **team member names** in the notebook submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (002C_1242)",
   "language": "python",
   "name": "002c_1242"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
